[{"content":"A theoretical framework aimed at explaining order in nature, and specifically self-organization and adaptation in non-living dynamical systems. It is inspired by the quest to understand the general drivers of the origins of life: why non-living matter might spontaneously turn into living matter. It\u0026rsquo;s a new and active area of research, with some of the core questions having been resolved, and others very much open—join our effort!\nRattling $\\mathcal{R}(x)$ is:\na property of a dynamical system\u0026rsquo;s state $x$ related to how quickly the system \u0026ldquo;leaves\u0026rdquo; the state easy to measure locally predictive of how likely you are to find the system in that state $x$ at long times useful far from equilibrium, as energy is useful at equilibrium related to how well state $x$ is \u0026ldquo;matched to\u0026rdquo; the system\u0026rsquo;s environment a way to explain order in nature ","date":"2023-09-07","id":0,"permalink":"/docs/background/whats-rattling/","summary":"\u003cp\u003eA theoretical framework aimed at explaining order in nature, and specifically self-organization and adaptation in non-living dynamical systems. It is inspired by the quest to understand the general drivers of the origins of life: why non-living matter might spontaneously turn into living matter. It\u0026rsquo;s a new and active area of research, with some of the \u003ca href=\"https://rattling.org/core-questions/\"\u003ecore questions\u003c/a\u003e having been resolved, and others very much open—\u003ca href=\"/contact/\"\u003ejoin our effort\u003c/a\u003e!\u003c/p\u003e","tags":[],"title":"What's rattling?"},{"content":"","date":"2024-11-30","id":1,"permalink":"/docs/background/","summary":"","tags":[],"title":"Background"},{"content":"","date":"2024-11-27","id":2,"permalink":"/tutorials/tutorial-for-physicists/","summary":"","tags":[],"title":"For physicists"},{"content":"Systems across physical scales and scientific domains exhibit order. Proteins fold, cells become tissues, and army ants form nests of their living bodies.1 However, to study order in nature, we must be careful in defining it: On the one hand, it refers to \u0026quot;concentration\u0026quot; – the system spending most of its time in a very small fraction of all possible configurations or states.2 However, many natural systems that are trapped in some configuration can still be \u0026ldquo;disordered.\u0026rdquo; So a second requirement must be introduced – that these few selected configurations are also recognizably \u0026quot;special.\u0026quot; This can come in the form of spatial order – such as with pattern formation like crystals or zebra stripes, or in the form of function – such as with living organisms. While equilibrium thermodynamics helps us understand a special case of order – self-assembly – the case of order, and especially functional order, in presence of energy sources remains largely open.\nThe Boltzmann distribution For systems in thermal equilibrium, there is a powerful explanation of order – specifically of self-assembly. A property of each state, called energy, determines the long-run fraction of time that the system spends in it. States with lower energy are favored to an extent determined by temperature, which is quantified by the Boltzmann distribution, the cornerstone of statistical mechanics:\n$$ P(x) = \\frac{1}{Z}\\; e^{-U(x)/T}. $$In this expression, $P(x)$ denotes the long-run (steady-state) probability that the system is found in state $x$. For ergodic systems, this is also the long-run fraction of time that the system spends in state $x$. The energy of $x$ is denoted by $U(x)$, $T$ is the temperature, and $Z$ is the normalization that ensures $P$ sums to one.\nLocal explains global The energy of a state is \u0026ldquo;local\u0026rdquo; in the sense that it can be determined without reference to any other state. For example, if $x$ represents a protein conformation, then $U(x)$ can be calculated from the positions of the molecules that it comprises. It isn\u0026rsquo;t important to know how one conformation becomes another, or how long it takes to do so. In contrast, the long-run fraction of time spent in each state is a \u0026ldquo;global\u0026rdquo; property of a system. Determining $P(x)$ generally requires observing the system for a long time, as it transitions through many other states. The Boltzmann distribution is remarkable because it explains a global thing $P(x)$ in terms of a local thing $U(x)$.\nThe problem Unfortunately, the Boltzmann distribution—and the explanation of order it provides—applies only to systems in thermal equilibrium. In particular, it does not apply to the many spectacular forms of order that living and driven systems exhibit. In fact, there can be no analogous explanation of order that applies to all nonequilibirum systems. This is the upshot of \u0026ldquo;Landauer\u0026rsquo;s blowtorch theorem.\u0026rdquo; Rattling theory circumvents this no-go theorem by weakening two its conditions, and thus makes possible a sort of nonequilibrium Boltzmann distribution, which we can then use to explain order – both the \u0026ldquo;concentration,\u0026rdquo; and the \u0026ldquo;functional specialness\u0026rdquo; of the selected states.\nThis kind of nest is called a bivouac.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWe mean the long-run fraction of time spent in a state. Implicit in our discussion is the assumption that the system reaches a steady or stationary state.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2023-09-07","id":3,"permalink":"/docs/background/explaining-order/","summary":"\u003cp\u003eSystems across physical scales and scientific domains exhibit order. Proteins fold, cells become tissues, and army ants form nests of their living bodies.\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e However, to study order in nature, we must be careful in defining it: On the one hand, it refers to \u003cem\u003e\u0026quot;\u003ca href=\"/core-questions/#strength-of-rattling-fine-tuning\"\u003econcentration\u003c/a\u003e\u0026quot;\u003c/em\u003e – the system spending most of its time in a very small fraction of all possible configurations or states.\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e However, many natural systems that are trapped in some configuration can still be \u0026ldquo;disordered.\u0026rdquo; So a second requirement must be introduced – that these few selected configurations are also recognizably \u003cem\u003e\u0026quot;\u003ca href=\"/core-questions/#environmental-information-encoding\"\u003especial\u003c/a\u003e.\u0026quot;\u003c/em\u003e This can come in the form of spatial order – such as with pattern formation like crystals or zebra stripes, or in the form of function – such as with living organisms. While equilibrium thermodynamics helps us understand a special case of order – self-assembly – the case of order, and especially functional order, in presence of energy sources remains largely open.\u003c/p\u003e","tags":[],"title":"Explaining order"},{"content":"There can be no analogue of the Boltzmann distribution that applies to all nonequilibrium systems. The purpose of this entry is to explain why. At this stage, we will split our discussion into sections intended for different readers. For those seeking a physical discussion, read on. For those seeking a mathematical analogue, skip to this section.\nA physicist\u0026rsquo;s perspective This classic no-go theorem from 1975 [Landauer75] was a response to the persistent hope in statistical physics to formulate a general \u0026ldquo;nonequilibrium Boltzmann distribution\u0026rdquo; – i.e., to express the nonequilibrium steady-state probability of any given state in terms of only local observables at that state. Landauer showed that this is in general impossible by providing a counter-example, illustrated in this picture:\nThis shows that we are able to change the steady-state probabilities in the wells $A$ and $B$ without changing anything locally in the neighborhoods of $A$ and $B$. All we need to do is apply localized heat (i.e., \u0026ldquo;a blowtorch\u0026rdquo;) at another location, here affecting the barrier between these two wells.\nThis proves that local information is not sufficient, in general, to predict the steady-state probability (as long as we are in nonequilibrium context).\nHow Rattling circumvents this Rattling, however, claims to do just this: it predicts the nonequilibrium steady-state $p_{ss}(x)$ in terms of the local Rattling value $\\mathcal{R}(x)$. This is possible for two reasons:\nthe Rattling result is approximate it is not fully general - but rather it is typical The blowtorch example falls outside the regime of applicability of Rattling as it is atypical - it is specifically constructed so as to exhibit strong non-local effects, i.e. it is an example of \u0026ldquo;adversarial fine-tuning.\u0026rdquo; One of our core research goals is to show that a broad class of naturally occurring systems avoid such regime, and thus satisfies Rattling. #general A mathematician\u0026rsquo;s perspective Recall the Boltzmann distribution from the entry on order. The fact that there can be no variant of the Boltzmann distribution that applies to all nonequilibrium steady states is analogous to a simple observation about Markov chains. Consider a continuous-time Markov chain that has a unique stationary distribution $\\pi$. The states of the chain represent the states of a physical system, and the transition rates between the states reflect the system\u0026rsquo;s dynamics.\nThe Markov chain analogue of the Boltzmann distribution would be an expression for the probability $\\pi(x)$ of a state $x$ as proportional to a \u0026ldquo;local\u0026rdquo; function of the rates of the chain. For example, consider the chain with four states depicted below.\nThe arrows depict unit rates between the indicated states, aside from $r_{34}$, which is the rate at which the chain transitions from state $3$ to state $4$. The relative probability of states $1$ and $2$ changes as the rate $r_{34}$ varies. This means that it is not possible to write $\\pi(x)$ as proportional to a function of the ingoing and outgoing rates of states $1$ and $2$. In other words, the stationary distribution of a Markov chain is generally a nonlocal function of its rates.\n","date":"2024-12-04","id":4,"permalink":"/docs/background/landauers-blowtorch/","summary":"\u003cp\u003eThere can be no analogue of the Boltzmann distribution that applies to all nonequilibrium systems. The purpose of this entry is to explain why. At this stage, we will split our discussion into sections intended for different readers. For those seeking a physical discussion, read on. For those seeking a mathematical analogue, skip to \u003ca href=\"#a-mathematicians-perspective\"\u003ethis section\u003c/a\u003e.\u003c/p\u003e","tags":[],"title":"Landauer's blowtorch"},{"content":"We claim that the Rattling hypothesis holds (i.e., predicts steady-state probability) for \u0026ldquo;typical\u0026rdquo; dynamical systems. On the one hand, \u0026ldquo;typical\u0026rdquo; here expresses an uncontrolled approximation – for a given system, it may be hard to determine a-priori how well Rattling will hold. On the other hand, it points to the intuition that Rattling should hold most of the time in real-world scenarios.\nSo what does \u0026ldquo;typical\u0026rdquo; mean exactly? Here we are referring to the concept of typicality – a related set of ideas and results that emerged over the last 50 years. Pankaj Mehta described it nicely in his 2024 paper \u0026ldquo;Theory is dead. Long live theory! For a 21st century statistical physics of life\u0026rdquo;:\nWigner showed that the statistical structure of nuclear resonances in the Uranium atom could be reproduced by modeling the Hamiltonian as a large symmetric random matrix.1 Winger’s conjectured that when a system becomes complex enough, many of its statistical properties will be “typical” and indistinguishable from a large random system subject to the same physical constraints as the object of study (i.e. is drawn from the same ensemble). In the case of Uranium, the relevant physical constraint Wigner identified was unitarity, which is why he considered symmetric random matrices to model the Hamiltonian of Uranium. More recently, this line of thinking has found great success in condensed matter theory in the context of Eigenstate Thermalization Hypothesis, which uses typicality to explain why even a single eigenstate of a sufficiently large manybody quantum system can be accurately described by statistical mechanics.2\nAs Freeman Dyson explained, this is a fundamentally “new kind of statistical mechanics, in which we renounce exact knowledge not of the state of a system, but the nature of the system itself.”3 What is nice about this approach is that it suggests a natural framework for thinking about living systems. Because it is agnostic to the origin of the constraints, it does not matter whether the relevant constraints originate from physical conservation laws, dynamics, geometry, energetics, evolutionary history, or even from specialized biological functions. For this reason, these ideas offer a promising avenue for thinking about how to model complex biological systems.\nIn the last few years this program has been successfully applied to understand many properties of complex microbial communities, suggesting that it can be operationalized to gain biological insights4. Moreover, modern generative models of proteins also naturally fall into this rubric. When stripped of all their bells and whistles, generative models of proteins are probabilistic models for sequence generation that respect the evolutionary and functional constrains embodied in amino acid conservation patterns.5\nThis is also the essence of \u0026ldquo;maximum entropy (MaxEnt) modeling\u0026rdquo; – where in the absence of having detailed measurements of the system parameters, we assume these to take their most likely values, constrained by some macroscopic or global parameters that we can measure. This technique has been applied in many systems, and even works to model neuron firing statistics and even neuron connectivity. In fact, MaxEnt is one way to derive the Boltzmann distribution, as well as our Rattling hypothesis (see Sec.2.2 in Supplement of our paper).\nAdversarial fine-tuning The main intuition behind \u0026ldquo;typicality\u0026rdquo; is that the system is large, complex, and did not suffer any adversarial fine-tuning. This is reminiscent of how a Neural Network can perfectly classify all \u0026ldquo;typical\u0026rdquo; cat images, unless someone adjusts the pixels slightly just right, so as to make it think it\u0026rsquo;s a dog (see here). In our case, such adversarial fine-tuning need not come from a malevolent agent, but from some pre-existing selection pressures. E.g., a biological organism is not a \u0026ldquo;typical\u0026rdquo; dynamical system as it has been fine-tuned by evolution. A pendulum is not a \u0026ldquo;typical\u0026rdquo; dynamical system because it\u0026rsquo;s too simple. A crystal is not because it\u0026rsquo;s too ordered (not complex enough).\nSo with this, the question arises: how typical exactly are dynamical systems where Rattling hypothesis holds? Can we construct a random ensemble representing all real-world dynamical systems, and then see what\u0026rsquo;s the probability of Rattling-friendly systems?\nGeneral guidelines While we cannot yet give rigorous a priori test that will tell us if Rattling hypothesis will hold, we generally expect it to hold for systems that:\nhave many interacting degrees of freedom have no particularly strong symmetries. This includes linearity, conservation of energy, etc. – as these could make the system act as a low-dimensional one or become integrable), and finally are in no way fine-tuned (such as by selection or evolution). Note since Rattling is most interesting for explaining self-organization, such self-organized configurations or dynamics will necessarily be selected and fine-tuned. So one must distinguish the dynamical system itself from the specific behaviors that it might exhibit – and so this requirement is for the former. More subtle point here is that in some systems self-organization may happen in stages or hierarchically, in which case some sub-space of the dynamics remains \u0026ldquo;typical,\u0026rdquo; while other degrees of freedom are fine-tuned. This scenario should still work with Rattling, but the details are an open question. Wigner, Eugene P. \u0026ldquo;Random matrices in physics.\u0026rdquo; SIAM review 9.1 (1967): 1-23.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSee this very nice review D’Alessio, Luca, et al. \u0026ldquo;From quantum chaos and eigenstate thermalization to statistical mechanics and thermodynamics.\u0026rdquo; Advances in Physics 65.3 (2016): 239-362.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nsee p2 Wigner, Eugene P. \u0026ldquo;Random matrices in physics.\u0026rdquo; SIAM review 9.1 (1967): 1-23\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFor example, see Goldford, Joshua E., et al. Science 361.6401 (2018): 469-474., Marsland III, Robert, Wenping Cui, and Pankaj Mehta. Scientific reports 10.1 (2020): 3308., Ho, Po-Yi, Benjamin H. Good, and Kerwyn Casey Huang. \u0026ldquo;Competition for fluctuating resources reproduces statistics of species abundance over time across wide-ranging microbiotas.\u0026rdquo; Elife 11 (2022): e75168.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis was a common perspective on generative protein models until the last five years. Recently, this narrative frame has retreated from popular conscience and been replaced by an almost exclusive focus on the machine learning architectures that enable one to learn such a distribution well.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-12-04","id":5,"permalink":"/docs/background/typicality/","summary":"\u003cp\u003eWe claim that the Rattling hypothesis holds (i.e., predicts steady-state probability) for \u0026ldquo;typical\u0026rdquo; dynamical systems. On the one hand, \u0026ldquo;typical\u0026rdquo; here expresses an uncontrolled approximation – for a given system, it may be hard to determine a-priori how well Rattling will hold. On the other hand, it points to the intuition that Rattling should hold most of the time in real-world scenarios.\u003c/p\u003e","tags":[],"title":"Typicality"},{"content":"Just as the equilibrium probability distribution can be expressed in terms of local energy $U(x)$ via the Boltzmann law:\n$$p_{eq}(x) = e^{-\\beta\\, U(x)}/Z.$$ So Rattling tell us that we can express non-equilibrium steady-states in terms of local Rattling value $\\mathcal{R}(x)$ as:\n$$p_{ss}(x) \\approx e^{-\\gamma\\, \\mathcal{R}(x)}/Z \\tag{1}.$$ This is a very powerful claim that has been sought by the nonequilibrium stat mech research community for decades, and so it comes at a price: the \u0026ldquo;$\\approx$\u0026rdquo; in this expression is a poorly-controlled approximation. It generally works for \u0026ldquo;typical\u0026rdquo; systems, but that notion so far lacks a formal definition. Some of the core questions in our research are thus understanding the assumptions and the practical generality of this approximation. #formal #general\nThe down-side of this is that currently, we don\u0026rsquo;t have a clear formal method (beyond heuristics) to a priori decide how well expression (1) will hold for a given system. For example, many key results in nonequilibrium statistical mechanics are derived from perturbative expansion for systems close to equilibrium, which is not the case here, and we have not identified any a priori small parameter that yields (1) in the limit. The upside is that this allows us to circumvent a core no-go theorem that held back progress in this area. In practice, we show that this expression holds for many systems arbitrarily far from equilibrium—regime where many other methods fail—thus allowing us to theoretically make predictions and engineer behaviors in these (see Examples).\n","date":"2024-12-04","id":6,"permalink":"/docs/background/nonequilibrium-boltzmann-distribution/","summary":"\u003cp\u003eJust as the equilibrium probability distribution can be expressed in terms of local energy $U(x)$ via the Boltzmann law:\u003c/p\u003e\n$$p_{eq}(x) = e^{-\\beta\\, U(x)}/Z.$$\u003cp\u003e\nSo Rattling tell us that we can express non-equilibrium steady-states in terms of local Rattling value $\\mathcal{R}(x)$ as:\u003c/p\u003e","tags":[],"title":"Nonequilibrium Boltzmann distribution"},{"content":"","date":"2024-11-27","id":7,"permalink":"/tutorials/tutorial-for-mathematicians/","summary":"","tags":[],"title":"For mathematicians"},{"content":"The upshot of Landauer\u0026rsquo;s blowtorch is that there can be no analogue of the Boltzmann distribution that applies to all nonequilibrium systems. As a consequence, it is not possible to analogously explain order in nonequilibrium systems. This suggests weakening the relation that the Boltzmann distribution constitutes, or narrowing the class of systems that must satisfy it. Rattling theory does both. We explain this point from a more mathematical perspective.\nThe following summarizes the key contribution of Calvert and Randall, PNAS (2024). We can think of the Boltzmann distribution as an expression for the stationary distribution $p_{ss}$ of a finite, continuous-time Markov jump process.1 In terms of a function $U$ of the ingoing and outgoing rates of state $x$, this expression is:\n$$ p_{ss} (x) \\propto e^{-U(x)}. $$We know that there is no \u0026ldquo;local\u0026rdquo; function $U$ that can satisfy this expression for all Markov jump processes. However, we can instead weaken the proportionality by observing that it is equivalent to perfect linear correlation between the logarithms of $p_{ss}$ and $-U$.\n$$ \\rho = \\frac{\\mathrm{Cov}(\\log p_{ss}(X),-U(X))}{\\sqrt{\\mathrm{Var}(\\log p_{ss}(X))\\mathrm{Var}(U(X))}}. $$ The Boltzmann distribution is satisfied if and only if $\\rho = 1$. A natural way to weaken the Boltzmann distribution is to allow $\\rho$ to be less than $1$. We can then ask about the kinds of Markov chains that have a value of $\\rho$ close to $1$, for a different local function of the rates. These chains are analogous to physical systems that satisfy an approximate analogue of the Boltzmann distribution, with a different local property of states in the place of energy.\nIn fact, the natural choice of the function that plays a role analogous to energy is the logarithm of the sum of rates leaving each state, and, for this choice, we can derive a formula for $\\rho$ in terms of properties of the Markov chain. To state this result requires some additional context, which a later entry will address. For now, we simply emphasize that generalizing the Boltzmann distribution means weakening the correlation that it normally entails, and finding a \u0026ldquo;local\u0026rdquo; function of the dynamics that produces strong correlation $\\rho$ simultaneously for many systems.\nMarkov jump processes on finite state spaces are commonly used to model the dynamics underlying the steady-state distributions of nonequilibrium physical systems. For example, see here.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIt is not important that $X$ is uniformly random. Any distribution on $X$ for which the correlation coefficient is defined will work.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-12-04","id":8,"permalink":"/docs/background/formalizing-rattling/","summary":"\u003cp\u003eThe upshot of \u003ca href=\"/landauers-blowtorch\"\u003eLandauer\u0026rsquo;s blowtorch\u003c/a\u003e is that there can be no analogue of the Boltzmann distribution that applies to all nonequilibrium systems. As a consequence, it is not possible to analogously explain order in nonequilibrium systems. This suggests weakening the relation that the Boltzmann distribution constitutes, or narrowing the class of systems that must satisfy it. Rattling theory does both. We explain this point from a more mathematical perspective.\u003c/p\u003e","tags":[],"title":"Formalizing rattling"},{"content":"","date":"2024-11-27","id":9,"permalink":"/tutorials/tutorial-for-life-scientists/","summary":"","tags":[],"title":"For life scientists"},{"content":"Q: So what exactly can Rattling theory tell me for my specific system, experiment or question? How do I use it?\nFirst you need to consider if you think the Rattling hypothesis should hold in your system. Generally, it should hold for \u0026ldquo;typical\u0026rdquo; systems – see these guidelines specifically. If this applies, then there are several things you can do:\nPredict relative likelihoods $$p_{ss}(x) \\approx e^{-\\gamma\\; \\mathcal{R}(x)}/Z$$ Rattling is powerful because it is easy to measure $\\mathcal{R}(x)$ – one just needs to initialize the system in state $x$ and see how fast it leaves that state on average. The generalization for systems with continuous configuration space is $\\mathcal{R}(x) = \\frac{1}{2} \\log \\det D(x)$, where $D(x)$ is the effective diffusion tensor at $x$, which can be similarly approximated from how fast it leaves $x$ (from taking a few short system trajectories starting at $x$, and calculating the covariance matrix – see details in our Science paper Materials and Methods, sec 3.2). Note that, empirically, it is easier to measure Rattling of a state than it is to measure energy of a state – since energy cannot be locally defined in terms of transition rates among the states [LINK].\n$$\\frac{p_{ss}(x)}{p_{ss}(y)} \\approx e^{-\\gamma \\,\\left[\\mathcal{R}(x) - \\mathcal{R}(y)\\right]} $$ The coefficient $\\gamma$ here is a system-specific constant, which will often be close to 1, but may deviate somewhat if the system doesn\u0026rsquo;t quite meet the mentioned guidelines (e.g., its configuration space is 6-dimensional rather than 100-dimensional). See here[LINK Jacob\u0026rsquo;s] for more details on what $\\gamma$ is. Either way, it is easy to estimate empirically from a few state-measurements and then make predictions for the rest.\nPredict absolute likelihoods? But to predict whether of not a system will self-organize, relative likelihoods are not enough, and so we need to find $Z$. $Z$, however, is not so easy to estimate empirically – since it depends globally on all configurations $x$. Unless we have empirical access to all system states, which is usually impractical, we need some theoretical calculation to find $Z$, and so some theoretical understanding of the global system properties. This is similar to finding the partition function in statistical mechanics, except that calculating energy of a state is well-known, while calculating its rattling is generally an open question.\nWe have some examples where we were able to successfully do this and predict the phase transition to self-organization (e.g., Vicsek model), but it\u0026rsquo;s not clear for which systems this is or isn\u0026rsquo;t possible. In other cases, it may be possible to combine theoretical and empirical methods to estimate $Z$ – by sampling a representative set of configurations and measuring their Rattling (here you need a sufficient understanding of your system to know how to choose that \u0026lsquo;representative set\u0026rsquo;).\nDestabilize by adding noise Predict self-organized configurations Control self-organized configurations (our paper examples - prop ratios, control, predict, etc\u0026hellip;\n","date":"2024-12-04","id":10,"permalink":"/docs/background/how-to-use-rattling/","summary":"\u003cp\u003eQ: So what exactly can Rattling theory tell me for my specific system, experiment or question? How do I use it?\u003c/p\u003e","tags":[],"title":"How to use rattling"},{"content":"","date":"2024-11-27","id":11,"permalink":"/tutorials/tutorial-for-engineers/","summary":"","tags":[],"title":"For engineers"},{"content":"Pavel and I have decided to make a website dedicated to the theory of rattling and its core open questions. We\u0026rsquo;ll be adding information to the docs over the coming weeks and months.\n","date":"2024-12-18","id":12,"permalink":"/blog/announcement/","summary":"\u003cp\u003ePavel and I have decided to make a website dedicated to the theory of rattling and its \u003ca href=\"/core-questions/\"\u003ecore open questions\u003c/a\u003e. We\u0026rsquo;ll be adding information to the \u003ca href=\"/docs/background/rattling/\"\u003edocs\u003c/a\u003e over the coming weeks and months.\u003c/p\u003e","tags":[],"title":"Announcement"},{"content":"","date":"2023-09-07","id":13,"permalink":"/blog/","summary":"","tags":[],"title":"Blog"},{"content":"","date":"2024-11-30","id":14,"permalink":"/docs/examples/","summary":"","tags":[],"title":"Examples"},{"content":"Here are several entries from an ever-growing list of examples:\nRandom energy model. A model of a random Boltzmann distribution on configurations of +/− spins. The configurations have energies that are independent normal random variables. Random chemical reaction system. A model of a chemical reaction system in which a fixed number of species react with random rate constants. The maximum molecularity of the reactions is also fixed. ","date":"2024-12-04","id":15,"permalink":"/docs/examples/summary/","summary":"\u003cp\u003eHere are several entries from an ever-growing list of examples:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"http://rattling.org/docs/examples/random-energy-model/\"\u003eRandom energy model\u003c/a\u003e. A model of a random Boltzmann distribution on configurations of +/− spins. The configurations have energies that are independent normal random variables.\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://rattling.org/docs/examples/random-chemistry/\"\u003eRandom chemical reaction system\u003c/a\u003e. A model of a chemical reaction system in which a fixed number of species react with random rate constants. The maximum molecularity of the reactions is also fixed.\u003c/li\u003e\n\u003c/ul\u003e","tags":[],"title":"Summary"},{"content":"One of the simplest, and also most general, models of equilibrium phase transition is the REM. Here, we consider the case where the energies of the accessible system states are normally distributed $\\{U(x)\\} \\sim \\mathcal{N}(0, \\sigma)$. This is motivated by the central limit theorem, and applies in cases where the state energy is a sum of many interaction energies that are so messy as to be essentially random – as may be the case for a spin-glass or a folded protein. This model is a prime example of using the assumption of typicality .\nThis model can then be shown to exhibit a phase transition. Since at equilibrium, $p_{eq}(x) \\propto e^{-\\beta\\, U(x)}$, we have a log-normal distribution of state probabilities, whose tale thickness is controlled by the inverse temperature $\\beta$ ($\\sigma$ above is considered a system-specific constant). When the tale becomes especially thick (at high $\\beta$ = low temperature), the probability distribution becomes dominated by the few states deep in the tale of low $U(x)$. This leads to a low-entropy fine-tuned selection of microstates reminiscent of the \u0026ldquo;frozen\u0026rdquo; or \u0026ldquo;solid\u0026rdquo; phase.\nRandom Rattling Model For Rattling, we might similarly expect $\\{\\mathcal{R}(x)\\} \\sim \\mathcal{N}(0, \\sigma)$ – but this needs to be checked more carefully [OPEN]. While energy of a state is compositional – it is the sum of all the subsystem interaction energies – the Rattling of a state does not always have this property (though sometimes it does – see here). Alternatively, we can remember its definition $\\mathcal{R} = \\frac{1}{2} \\log \\det \\mathcal{C}$ in terms of the correlation matrix $\\mathcal{C}_{ij} = \\left\u003c\\dot{x}_i, \\dot{x}_j\\right\u003e$. Here, looking for the typical distribution of Rattling in a complex system, puts us in the territory of random matrix theory: if we can assume that the correlations are so messy as to be essentially random, then we are looking for log det of a random symmetric positive semi-definite matrix. This paper showed that this may actually be Gaussian distributed – taking us back to where we started. In general, however, this will depend on the specific random matrix ensemble, and so we need to consider carefully our specific case.\nEmpirically, however, we see that Rattling does tend to be normally distributed for many complex dynamical systems. This means that the REM results carry over directly, predicting two possible phases for dynamical system steady-states: when the variance of Rattling values across system states is high, we\u0026rsquo;ll expect probability to concentrate strongly in the few least-Rattling states in the tail of the distribution. Besides giving a nice example of a Rattling-driven phase transition, this also addresses one of the core questions of this research [FIX link] – whether Rattling effects are typically strong enough to create strong fine-tuning.\n","date":"2024-12-04","id":16,"permalink":"/docs/examples/random-energy-model/","summary":"\u003cp\u003eOne of the simplest, and also most general, models of equilibrium phase transition is the \u003ca href=\"https://en.wikipedia.org/wiki/Random_energy_model\"\u003eREM\u003c/a\u003e. Here, we consider the case where the energies of the accessible system states are normally distributed $\\{U(x)\\} \\sim \\mathcal{N}(0, \\sigma)$. This is motivated by the central limit theorem, and applies in cases where the state energy is a sum of many interaction energies that are so messy as to be essentially random – as may be the case for a spin-glass or a folded protein. This model is a prime example of using the assumption of \u003ca href=\"https://rattling.org/docs/background/typicality\"\u003etypicality\u003c/a\u003e .\u003c/p\u003e","tags":[],"title":"Random energy model"},{"content":"","date":"2023-09-07","id":17,"permalink":"/docs/examples/random-chemistry/","summary":"","tags":[],"title":"Random chemistry"},{"content":"","date":"2024-11-30","id":18,"permalink":"/docs/research-directions/","summary":"","tags":[],"title":"Research directions"},{"content":"[For broader context, see Rattling Thermodynamics] One limitation of the Rattling framework $p_{ss}(x) \\approx e^{-\\gamma\\; \\mathcal{R}(x)}/Z \\tag{1}$ is that for arbitrary far-from-equilibrium nonlinear dynamical systems, it is difficult to analytically estimate Rattling of any given state $\\mathcal{R}(x)$. So we often need to explicitly measure $\\mathcal{R}(x)$. This still allows us to predict global steady-state in terms of only local measurements of Rattling, and so is already practically useful, but the cases where we can predict $\\mathcal{R}(x)$ analytically allow for much more prediction and control.\nComposability of energy This can be compared to the equilibrium counter-part, the Boltzmann distribution $p_{eq}(x) = e^{-\\gamma\\; U(x)}/Z$ , where the state energy $U(x)$ can often be calculated analytically even for very complex equilibrium systems. Now notice that there is no fundamental sense in which state energy should be any more \u0026ldquo;natural\u0026rdquo; or \u0026ldquo;simple\u0026rdquo; to deal with than Rattling – the latter is in fact easier to access experimentally using only a single local measurement (by just checking the state exit rate). The main reason why energy $U(x)$ is so useful in equilibrium systems is because it is conserved (which leads to the Boltzmann distribution), and composable (which means that the energy of any complex system state is just the sum of all constituent subsystem energies and interaction energies).\nSo if there was a way to calculate the Rattling of a complex composite system from knowing something about the Rattling of the component dynamics and their interactions, then our framework would be comparable in predictive power to the equilibrium stat mech.\nComposability of Rattling? Independent systems While this is probably too much to ask in the general case, some progress can likely be made. To begin, we notice that for non-interacting subsystems $A$ and $B$, steady-state probability distribution factors, and so Rattling is indeed composable: $p_{ss}(x_{AB}) = p_{ss}(x_A)\\; p_{ss}(x_B) \\approx e^{-\\gamma\\; \\left(\\mathcal{R}(x_A)+\\mathcal{R}(x_B)\\right)}/Z$. We can also verify this from directly calculating Rattling of the composite system: remembering that for continuous systems $\\mathcal{R}(x_{AB}) = \\frac{1}{2} \\log \\det \\mathcal{C}(x_{AB})$, in terms of the covariance matrix $\\mathcal{C}$, which will be block-diagonal for a system composed of two independent subsystems, giving $\\det \\mathcal{C}_{AB} = \\det \\mathcal{C}_A\\,\\det \\mathcal{C}_B$, thus consistently recovering $\\mathcal{R}(x_{AB}) = \\mathcal{R}(x_{A}) + \\mathcal{R}(x_{B})$.\nThis suggest one possible research direction:\nPerturbatively turn on weak interactions, and see how this affects the composite Rattling values, look for general principles that may extend beyond the perturbative regime. This is similar to the treatment of weakly-coupled field theories, which may, in fact, be a good example to study this on (see Anharmonic Net example [FIX link]). Stochastic systems Another example where we can use composition is if one of the systems is so chaotic that it can be estimated as state-independent noise: $\\mathcal{R}_{B} = const$, and so that interactions just serve to \u0026ldquo;raise the temperature\u0026rdquo; of the composite. This suggests another research direction:\nFor any interaction strengths, look for ways to decompose the composite system Rattling in terms of subsystem Rattlings and some additional \u0026ldquo;interaction contribution\u0026rdquo; – which we can always define as $\\mathcal{R}_{int} \\equiv \\mathcal{R}_{AB}-\\mathcal{R}_A-\\mathcal{R}_B$. The question is then whether this $\\mathcal{R}_{int}$ has some nice properties and may be analytically predicted in some cases. One hypothesis would be that it may be estimated in terms of the relationship between the symmetries in the driving force and the symmetries of the composite system state $AB$ – so in the above case of \u0026ldquo;$B$ as noise,\u0026rdquo; there are no compatible symmetries, and so $\\mathcal{R}_{int}=0$. Composing over driving force components Yet another interesting approach comes from composing Rattling not out of its subsystems, as is done with energy, but rather out of the different components of its driving force. This was successfully demonstrated in our Science paper, fig 2 and 3, [FIX link] where it allowed us to analytically predict and engineer the steady-state behaviors of a robot swarm.\n\u0026ldquo;Analytic continuation\u0026rdquo; of Rattling landscape More generally, this suggests another route to analytically predicting Rattling – by measuring it for some parameter regime of a system, and then using that to predict how the Rattling landscape $\\mathcal{R}(x)$ will evolve as we adjust the parameters. Though perhaps this path seems more system specific, and less likely to yield a general \u0026ldquo;cookbook.\u0026rdquo;\n","date":"2024-12-04","id":19,"permalink":"/docs/research-directions/predicting-rattling/","summary":"\u003cp\u003e[For broader context, see \u003ca href=\"https://rattling.org/docs/background/rattling-thermo/\"\u003eRattling Thermodynamics\u003c/a\u003e]\nOne limitation of the Rattling framework $p_{ss}(x) \\approx e^{-\\gamma\\; \\mathcal{R}(x)}/Z \\tag{1}$  is that for arbitrary far-from-equilibrium nonlinear dynamical systems, it is difficult to analytically estimate Rattling of any given state $\\mathcal{R}(x)$. So we often need to explicitly measure $\\mathcal{R}(x)$. This still allows us to predict global steady-state in terms of only local measurements of Rattling, and so is already practically useful, but the cases where we can predict $\\mathcal{R}(x)$ analytically allow for much more prediction and control.\u003c/p\u003e","tags":[],"title":"Predicting rattling"},{"content":"Add fig from Science SI, and discuss Bayes encoding (link paper)\n","date":"2024-12-04","id":20,"permalink":"/docs/research-directions/learning-from-the-drive/","summary":"\u003cp\u003eAdd fig from Science SI, and discuss Bayes encoding (link paper)\u003c/p\u003e","tags":[],"title":"Learning from the drive"},{"content":"Rattling 1 paper, non-ergodic relaxation\n","date":"2024-12-04","id":21,"permalink":"/docs/research-directions/rattling-for-transients/","summary":"\u003cp\u003eRattling 1 paper, non-ergodic relaxation\u003c/p\u003e","tags":[],"title":"Rattling for transients"},{"content":"Rattling plays the same role in non-equilibrium steady-state distributions (for typical systems) as energy does in the Boltzmann distribution. This suggests comparing properties of Rattling $\\mathcal{R}$ with those of energy, to see if $\\mathcal{R}$ can be as powerful as energy in various contexts. Below we present a more intuitive (physics) and more formal (math) perspective on this comparison:\nPhysics perspective First notice that there is no fundamental sense in which state energy should be any more \u0026ldquo;natural\u0026rdquo; or \u0026ldquo;simple\u0026rdquo; to deal with than Rattling. Energy is more familiar to us, but ratting is in fact easier to access experimentally using only local measurements - by just checking the state exit rate. In contrast, there is actually no simple local way to measure the energy of a given state (partly because only relative energies are physically meaningful - so there is no absolute quantity to measure). The main reason why energy $U(x)$ is so useful in equilibrium systems is because it is conserved (which leads to the Boltzmann distribution), and composable (which means that the energy of any complex system state is just the sum of all constituent subsystem energies and interaction energies). This composability of energy often allows us to analytically calculate the energy of a state, even if we can\u0026rsquo;t measure it directly. One core question here is whether Rattling of a state is as easy to calculate as state energy. At first this seems challenging, but some progress can, surprisingly, be made – see discussion on this here for some initial steps.\nSee the page on rattling thermodynamics for research directions that this opens.\nMore formally Jacob\u0026hellip;\n","date":"2024-12-04","id":22,"permalink":"/docs/research-directions/rattling-vs-energy/","summary":"\u003cp\u003eRattling plays the same role in non-equilibrium steady-state distributions (for \u003ca href=\"/docs/background/typicality\"\u003etypical\u003c/a\u003e systems) as energy does in the Boltzmann distribution. This suggests comparing properties of Rattling $\\mathcal{R}$ with those of energy, to see if $\\mathcal{R}$ can be as powerful as energy in various contexts.\nBelow we present a more intuitive (physics) and more formal (math) perspective on this comparison:\u003c/p\u003e","tags":[],"title":"Rattling vs energy"},{"content":"Rattling is just about re-scaling time. Strong parallels to entropy\n","date":"2023-09-07","id":23,"permalink":"/docs/research-directions/rattling-vs-entropy/","summary":"\u003cp\u003eRattling is just about re-scaling time. Strong parallels to entropy\u003c/p\u003e","tags":[],"title":"Rattling vs entropy"},{"content":"Rattling gives us a \u0026ldquo;Nonequilibrium Boltzmann Distribution\u0026rdquo; (a statement that must be carefully qualified and still needs further development and clarification, see link). This is something that has been sought by the nonequilibrium stat mech community for decades, and so if we suppose that we have this, it opens many possibilities. One natural next step is to try to translate the plethora of phenomena and results known from equilibrium thermodynamics to nonequilibrium systems, and verify them on experimental and numerical examples. Let\u0026rsquo;s now go through a few research directions in more detail:\nEnergy vs Rattling Rattling plays the same role in non-equilibrium steady-state distributions (for typical systems) as energy does in the Boltzmann distribution. This suggests comparing properties of Rattling $\\mathcal{R}$ with those of energy, to see if $\\mathcal{R}$ can be as powerful as energy in various contexts. See here for details.\nHeat engines? Another property of energy that is important is that it\u0026rsquo;s locally conserved, and can flow among degrees of freedom (e.g., in the form of heat or work). Is there some equivalent of this for Rattling? This question is wide open at the moment. #open So much of our intuitions are based on energy that it\u0026rsquo;s tough to translate to the very different context of Rattling - but if we suspend our habits and take a fresh perspective, we may be surprised. If this connection can be made, it would open up the territory of heat engines (e.g., Carnot cycle) for translation into Rattling context.\nEntropy-energy tradeoff In statistical mechanics, the tradeoff between energy and entropy is responsible for many of the most interesting predictions, such as phase transitions. This can directly translate to Rattling-entropy tradeoff, since when we coarse-grain the microstate distribution $p_{ss}(x) \\approx e^{-\\gamma\\; \\mathcal{R}(x)}$, we will naturally get $p_{ss}(X) \\approx e^{-\\gamma\\; \\mathcal{R}(X) + S(X)}$, where $S(X)$ is the entropy of the macrostate $X$. We explicitly showed how this calculation can be carried out, and how it can predict the phase transition in the Vicsek model of flocking – see here. While it seemed to work in that example, the broader concept raises a core question of how Rattling acts under coarse-graining (what is $\\mathcal{R}(X)$ and how it relates to $\\mathcal{R}(x)$ in general), whether there is a preferred level of granularity where Rattling is most accurate for steady-state prediction, and how to identify such granularity in new systems. #open\nRandom Energy Model (REM) This is a very simple and general model of equilibrium phase transitions in complex systems, like spin-glasses and proteins. It carries over nicely to the case of Rattling, showing us that we can similarly expect Rattling to cause such phase transitions in a wide variety of complex driven systems, and thus create strong fine-tuned selection of few least-Rattling states. See here for details.\nRattling-energy interactions The other interesting question is what happens in driven systems that have both, a strong energy landscape over their states, as well as a Rattling landscape. For strong driving, we might expect the driving to overwhelm any internal energies, making the energy-landscape hard to even define meaningfully (since energy is not conserved). However, in practice, the driving may not be strong enough to overwhelm all energetic effects, and we can somehow usefully define it.\nIn general, we don\u0026rsquo;t expect a simple answer in these cases. Namely, we do NOT expect them to combine simply as $p_{ss}(x) \\approx e^{-\\gamma\\; \\mathcal{R}(x) - \\beta \\; U(x)}/Z \\tag{1}$. In our first paper on Rattling, we explored this question in a very simple 1D setting – and were able to verify some analytical predictions, see fig. 2 and 4 in this paper. In more complex and general contexts, this question is still open. #open\n","date":"2024-12-04","id":24,"permalink":"/docs/research-directions/rattling-thermodynamics/","summary":"\u003cp\u003eRattling gives us a \u0026ldquo;\u003ca href=\"https://rattling.org/docs/background/neq-boltzmann/\"\u003eNonequilibrium Boltzmann Distribution\u003c/a\u003e\u0026rdquo; (a statement that must be carefully qualified and still needs further development and clarification, see link). This is something that has been sought by the nonequilibrium stat mech community for decades, and so if we suppose that we have this, it opens many possibilities. One natural next step is to try to translate the plethora of phenomena and results known from equilibrium thermodynamics to nonequilibrium systems, and verify them on experimental and numerical examples.\nLet\u0026rsquo;s now go through a few research directions in more detail:\u003c/p\u003e","tags":[],"title":"Rattling thermodynamics"},{"content":"","date":"2023-09-07","id":25,"permalink":"/docs/","summary":"","tags":[],"title":"Learn"},{"content":"Typicality of rattling Rattling $\\mathcal{R}(x)$ is a local property of a state $x$. How common are systems for which it strongly (anti)-correlates with the global steady-state distribution $p_{ss}(x)$?\nWhile we have many examples of systems for which the rattling correlation holds in this sense, we still need to evaluate how commonly it holds among real-world systems.\nThere is also a mathematical version of this question, stemming from a formula for the rattling correlation in Markov jump processes, with recent results.\nStrength of rattling fine-tuning In systems where rattling holds, can it produce a strongly concentrated $p_{ss}(x)$ distribution?\nTo explain the remarkable degree of order found in nature, the fine-tuning due to rattling needs to be very strong, requiring $\\mathcal{R}(x)$ to vary over several orders of magnitude. Is this possible and likely to happen in natural systems?\nSee examples, specifically the discussion in REM.\nEnvironmental information encoding When can we say that the dynamical system is \u0026ldquo;learning\u0026rdquo; from its environment?\nOrder in nature does not just mean selection of rare configurations—these configurations must also be recognizably \u0026ldquo;special.\u0026rdquo; To talk about adaptation, this specialness must be in the way the configurations reflect the information in their external driving forces or environment.\nSee this page for some evidence and ideas on how to approach this.\nRattling for transients Rattling is typically predictive of steady-state probabilities. Can it tell us anything about the transient dynamics?\nFor questions of adaptation and origins of life, the key concern isn\u0026rsquo;t whether life can be stable, but whether it can emerge on a reasonable timescale. So we must ask not only about the steady-state, but also about relaxation times and dynamics.\nOne way to explore this question is to see if the theory of rattling for Markov jump processes, which concerns the stationary distribution, also applies in some way to finite-time distributions.\nThis question is wide open, see this page for what we know so far.\nRattling workflow What is a principled and practical way to calculate rattling or estimate it from data?\nWhat can rattling do for you in your specific experiment or question?\nSee here for details on experimental uses, and here for discussion on calculating or predicting it.\nMore specific questions: Rattling under transformations The extent to which rattling predicts a system\u0026rsquo;s steady-state distribution depends on how the states are defined. If you transform the state space, say, by redefining your degrees of freedom or by coarse-graining, how does rattling change?\nRattling under composition Rattling is a property of a state of a system. For systems that are groups of interacting elements (e.g., a flock of birds), a state might consist of the positions of all the elements (birds) in the group. Can we predict the rattling of such collective system state (flock) based on the rattling or dynamics of the individual elements (birds)? In other words, how can we go \u0026ldquo;one level down\u0026rdquo; in composite systems?\nNote that rather than being just about coarse-graining (as in the question above), this is a question about \u0026ldquo;interaction rattling\u0026rdquo;—in parallel to \u0026ldquo;interaction energy\u0026rdquo; that we have in statistical mechanics. Perhaps even could there be some kind of renormalization group flow for rattling?\nSee here for further discussion.\nHierarchical self-organization Most real-world examples of self-organization proceed in stages or hierarchically—e.g., first cells form, then multi-cellular organisms, then organism groups or societies, etc. While the rattling framework should be able to explain and predict such stepwise self-organization, we have yet to develop and study a clear example of this. Some interesting subtleties might arise here because intermediate stages present a scenario where some degrees of freedom are fine-tuned, while others remain \u0026ldquo;typical.\u0026rdquo;\nSee Examples.\nA priori test of rattling How do we know for a given system if we should use rattling or not?\nIs it possible to use some simple measurements of the system to a priori know the success or failure of rattling to predict a system\u0026rsquo;s steady-state distribution?\nConnections to other frameworks How does the rattling framework relate to other frameworks on nonequilibrium steady-states and self-organization?\nEspecially, e.g., stochastic thermodynamics.\n","date":"2024-12-06","id":26,"permalink":"/core-questions/","summary":"\u003ch2 id=\"typicality-of-rattling\"\u003eTypicality of rattling\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eRattling $\\mathcal{R}(x)$ is a local property of a state $x$. How common are systems for which it strongly (anti)-correlates with the global steady-state distribution $p_{ss}(x)$?\u003c/strong\u003e\u003c/p\u003e","tags":[],"title":"Core Questions"},{"content":"Contact us by sending an email to contact@rattling.org.\n","date":"2024-11-27","id":27,"permalink":"/contact/","summary":"\u003cp\u003eContact us by sending an email to \u003ca href=\"mailto:contact@rattling.org\"\u003econtact@rattling.org\u003c/a\u003e.\u003c/p\u003e","tags":[],"title":"Contact"},{"content":"Here\u0026rsquo;s a talk about the Markov chain theory of rattling that Jacob gave during a visit to the Santa Fe Institute in 2024. The corresponding paper is Calvert–Randall (2024).\nPavel discussed the Chvykov et al. (2021) paper on this episode of the podcast for the journal Science. He also gave a talk at the University of Michigan in 2021.\nHere\u0026rsquo;s an earlier talk that Pavel gave at the Santa Fe Institute in 2019.\n","date":"2024-11-27","id":28,"permalink":"/talks/","summary":"\u003cp\u003eHere\u0026rsquo;s a talk about the Markov chain theory of rattling that Jacob gave during a visit to the Santa Fe Institute in 2024. The corresponding paper is \u003ca href=\"/docs/reference/calvertrandall-paper/\"\u003eCalvert–Randall (2024)\u003c/a\u003e.\u003c/p\u003e","tags":[],"title":"Talks"},{"content":"","date":"2024-11-27","id":29,"permalink":"/tutorials/","summary":"","tags":[],"title":"Tutorials"},{"content":"The site This site is about rattling theory, which is an emerging explanation of self-organization. Its purpose is to explain the theory, collect and share examples, and provide guides for the use of rattling in research.\nIts authors We\u0026rsquo;re Jacob Calvert and Pavel Chvykov.\n","date":"2024-11-27","id":30,"permalink":"/about/","summary":"\u003ch1 id=\"the-site\"\u003eThe site\u003c/h1\u003e\n\u003cp\u003eThis site is about rattling theory, which is an emerging explanation of self-organization. Its purpose is to explain the theory, collect and share examples, and provide guides for the use of rattling in research.\u003c/p\u003e","tags":[],"title":"About"},{"content":"If you\u0026rsquo;d like to understand the background of rattling theory, how the theory applies to a variety of examples, or how you can use it in your own work, check out the docs.\n","date":"2024-11-27","id":31,"permalink":"/learn/","summary":"\u003cp\u003eIf you\u0026rsquo;d like to understand the background of rattling theory, how the theory applies to a variety of examples, or how you can use it in your own work, check out the \u003ca href=\"/docs/background/rattling/\"\u003edocs\u003c/a\u003e.\u003c/p\u003e","tags":[],"title":"Learn"},{"content":"","date":"2023-09-07","id":32,"permalink":"/privacy/","summary":"","tags":[],"title":"Privacy Policy"},{"content":"","date":"2023-09-07","id":33,"permalink":"/","summary":"","tags":[],"title":"How does order arise in nature?"},{"content":"","date":"0001-01-01","id":34,"permalink":"/categories/","summary":"","tags":[],"title":"Categories"},{"content":"","date":"0001-01-01","id":35,"permalink":"/contributors/","summary":"","tags":[],"title":"Contributors"},{"content":"","date":"0001-01-01","id":36,"permalink":"/tags/","summary":"","tags":[],"title":"Tags"}]